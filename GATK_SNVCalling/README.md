# Introduction
- All data were aligned to reference genome GRCh37 (hg19) with bwa 0.7.17 mem. <br>
- We followed the GATK (4.0.5.1) SNV calling best practices (https://gatk.broadinstitute.org/hc/en-us/articles/360035535932-Germline-short-variant-discovery-SNPs-Indels-) and only two parameters were changed to accommodate computational resources (--max_records_in_ram and --max-reads-per-alignment-start). <br>
- The major procedures include 1) remove duplication with Picard, 2) Recalibrate Base Quality; 3) Combine all samples together and run HaplotypeCaller and generate GVCF; 4) Consolidate GVCF and Joint call all variants with CombineGVCF and GenotypeGVCF; <br>
- Finally, all high-quality variants were preserved if 1) the quality score is high (QD>=5,QUAL>=50, FS<=60, MQ>=40, MQRankSum>=-12.5, ReadPosRankSum=>-8, SOR<=3),  and 2) there are no SNV clusters within 10 bp (--cluster-size 3, --cluster-window-size 10). 

# Pipeline
### Layout of your folder:
- Pipeline: for all your pipeline
- FASTQ: for all you raw FASTQ files and all following BAM files;
- CombineGVCF: temporay folder for CombineGVCF 
- GenotypeGVCF: temporay folder for GenotypeGVCF
- Final_VCF:  Final_VCF folder

### Prepare

Create new folder that are going to use. 

### Step 1, Mapping with BWA, S1.bwa.sh
Assume you have downloaded the reference and created the index file. If not, use bwa index YOUR_REFERENCE_FASTA_FILE and find more details here: http://bio-bwa.sourceforge.net/bwa.shtml

Input for mapping here is FASTQ. If your input is BAM file, please go to https://github.com/CancerGenome/Bioinformatics_Pipeline/tree/master/VarScan_SomaticSNVCalling Step One.

- One Step to generate the BWA running shell.

> perl S1.bwa.pl BAM_LIST > S1.bwa.sh

> sh S1.bwa.sh 

Where BAM_LIST format is: FULL PATH BAM, one line each. 
Please replace /home/yulywang/db/human/hs37d5 and /home/yulywang/db/human/hs37d5.fai with your own reference index and index fai.

Breakdown for S1.bwa.sh steps:

- Map with BWA, you should replace with your own index file for hs37d5
> bwa7.17 mem -R "@RG\tID:1200C\tPL:Illumina\tLB:1200C\tDS:pe::0\tDT:2021-06-02\tSM:1200C\tCN:U_Michigan_YuWang_Ganesh_Lab" -t 4 -k 20 -w 105 -d 105 -r 1.3 -c 12000 -A 1 -B 4 -O 6 -E 1 -L 6 -U 18 /home/yulywang/db/human/hs37d5 TEST_R1.fastq.gz TEST_R2.fastq.gz | gzip -3 > TEST.sam.gz

- Convert SAM to BAM, replace your own index.fai file
> gzip -dc TEST.sam.gz | samtools view --threads 4 -b1ht /home/yulywang/db/human/hs37d5.fa.fai - > TEST.unsort.bam;

- Sort and index BAM files 
> samtools sort -m 1600M --threads 4 -O BAM -o TEST.sort.bam TEST.unsort.bam;samtools index TEST.sort.bam;

- Clean up everythings
> rm TEST.sortid.bam; rm TEST.sam.gz; rm TEST.unsort.bam; 

### Step 2, Mark Duplicaton with Picard, S2.picard.sh

Run picard to label duplication.

> picard.sh TEST.sort.bam TEST

where TEST.sort.bam is sorted bam file generated by BWA, TEST is the prefix for all OUTPUT

### Step 3, Base Quality Recalibration and GVCF Infer, S3.BQSR_GVCF.sh

Run Base Quality Recalibartion and Call GVCF 

> GATK_BQSR_GVCF.sh TEST.rmdup.bam TEST

where TEST.rmdup.bam is duplication-labelled bam file generated by Picard, TEST is the prefix for all OUTPUT

### Caveat: Samples are not dependent for analysis above. For analysis below, you need to wait for all samples finished.

### Step 4 - 8 Combine GVCF, Merge Genotype, Hard Filtering and Annotation, S5_8_CombineGVCF_Merge_GenotypeGVCF_HardFilter_Annovar.sh

After all samples have their own GVCF files, combine GVCF for each 30M, concat together, hard filtering for variants, and annotation. One command to run Step 4-8:

> ls *.g.vcf.gz > GVCF.list

> ./GATK_CombineGVCF_Merge_GenotypeGVCF_HardFilter_Annovar.pl -l GVCF.list > S5_8_CombineGVCF_Merge_GenotypeGVCF_HardFilter_Annovar.sh

> sh GATK_CombineGVCF_Merge_GenotypeGVCF_HardFilter_Annovar.sh

Breakdown for each step details:

- Step Five: CombineGVCF by each chrosome 30M, Input: all g.vcf.gz. Because the sample size is always big, try too analyze only 30M intervals each time. The PERL script above will generate the whole genome with 30M internals. 

> gatk4.0.5 CombineGVCFs -R /home/yulywang/db/human/hs37d5.fa  -V /home/yulywang/1.g.vcf.gz  -V /home/yulywang/2.g.vcf.gz  -O ../CombineGVCF/1_1_30000000.g.vcf.gz -L 1:1-30000000 ; 

- Step Six: GenotypeGVCF on each chromosome 30M
> gatk4.0.5 GenotypeGVCFs -R /home/yulywang/db/human/hs37d5.fa --dbsnp /home/yulywang/db/dbsnp/dbsnp_138.hg19.excluding_sites_after_129.vcf.gz -V  ../CombineGVCF/1_1_30000000.g.vcf.gz -O ../GenotypeGVCF/1_1_30000000.vcf.gz -L 1:1-30000000 ; 

- Step Seven: Variant Calibration Hard Filter;
 Hard Filter: 
        SNPs:
		-filter QD < 5.0 -filter QUAL < 50.0 --cluster-size 3  --cluster-window-size 10
	-filter SOR > 3.0 -filter FS > 60.0 -filter MQ < 40.0 -filter MQRankSum <-12.5 -filter ReadPosRankSum < -8.0
	     INDELS:
		-filter QD < 5.0 -filter QUAL < 50.0 --cluster-size 3  --cluster-window-size 10
		-filter FS > 60.0 -filter ReadPosRankSum < -20.0

- Step Eight: Annovar annotation 
> GATK_HardFilter_Annovar.sh ../GenotypeGVCF/1_1_30000000.vcf.gz ../GenotypeGVCF/1_1_30000000.vcf.gz 

### Step 9 Concat all together.

Concat all together: 
> sh S9.Concat_after_Annovar.sh
